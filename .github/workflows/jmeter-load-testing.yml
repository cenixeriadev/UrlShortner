name: JMeter Load Testing

on:
  push:
    branches: [ main]
  pull_request:
    branches: [ main ]
  schedule:
    # Ejecutar pruebas de carga todos los días a las 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
        type: string
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        default: '50'
        type: string
      ramp_up_time:
        description: 'Ramp-up time in seconds'
        required: false
        default: '60'
        type: string

env:
  JAVA_VERSION: '21'
  JAVA_DISTRIBUTION: 'corretto'
  JMETER_VERSION: '5.6.3'
  API_BASE_URL: 'http://localhost:8081/api/v1'

jobs:
  load-testing:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: ${{ env.JAVA_DISTRIBUTION }}

      - name: Cache Maven dependencies
        uses: actions/cache@v4
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2
      - name: Create .env file for testing
        run: |
          cat > .env << EOF
          DB_HOST=localhost
          DB_HOST=localhost
          DB_PORT=5432
          DB_NAME=urlshort
          DB_USERNAME=testuser
          DB_PASSWORD=testpass
          REDIS_HOST=localhost
          REDIS_PORT=6379
          SERVER_PORT=8081
          EOF
      - name: Copy .env to backend for runtime
        run: |
          cp .env backend/.env
      - name: Make mvnw executable and compile
        working-directory: ./backend
        run: |
          chmod +x ./mvnw
          ./mvnw clean package -DskipTests
      - name: Start services with Docker Compose
        run: |
          # Crear docker-compose para testing si no existe
          if [ ! -f "docker-compose.test.yml" ]; then
            cat > docker-compose.test.yml << 'EOF'
          version: '3.8'
          services:
            postgres:
              image: postgres:16
              environment:
                POSTGRES_DB: urlshort
                POSTGRES_USER: testuser
                POSTGRES_PASSWORD: testpass
              ports:
                - "5432:5432"
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U testuser -d urlshortener"]
                interval: 10s
                timeout: 5s
                retries: 5
          
            redis:
              image: redis:7-alpine
              ports:
                - "6379:6379"
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 10s
                timeout: 5s
                retries: 5
          
            backend:
              build:
                context: ./backend
                dockerfile: Dockerfile
              environment:
                - DB_HOST=localhost
                - DB_PORT=5432
                - DB_NAME=urlshort
                - DB_USERNAME=testuser
                - DB_PASSWORD=testpass
                - REDIS_HOST=localhost
                - REDIS_PORT=6379
                - SERVER_PORT=8081
                - SPRING_PROFILES_ACTIVE=test
              env_file:
                - .env
              ports:
                - "8081:8081"
              depends_on:
                postgres:
                  condition: service_healthy
                redis:
                  condition: service_healthy
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:8081/actuator/health"]
                interval: 30s
                timeout: 10s
                retries: 5
          EOF
          fi
          
          # Levantar los servicios
          docker compose -f docker-compose.test.yml up -d postgres redis
          
          # Esperar a que los servicios estén listos
          echo "Waiting for services to be healthy..."
          timeout=120
          while [ $timeout -gt 0 ]; do
            if docker compose -f docker-compose.test.yml ps | grep -q "healthy"; then
              echo "Database and Redis are healthy"
              break
            fi
            sleep 2
            timeout=$((timeout-2))
          done

      - name: Build and start backend
        run: |
          # Ahora construir la imagen del backend
          docker compose -f docker-compose.test.yml build backend
          
          # Levantar el backend
          docker compose -f docker-compose.test.yml up -d backend
          
          # Esperar a que la aplicación esté lista
          echo "Waiting for backend to be ready..."
          timeout=120
          while [ $timeout -gt 0 ]; do
            if curl -f http://localhost:8081/actuator/health > /dev/null 2>&1; then
              echo "Backend is ready!"
              break
            fi
            sleep 2
            timeout=$((timeout-2))
          done
          
          if [ $timeout -le 0 ]; then
            echo "Backend failed to start"
            echo "=== Backend logs ==="
            docker compose -f docker-compose.test.yml logs backend
            echo "=== Service status ==="
            docker compose -f docker-compose.test.yml ps
            exit 1
          fi

      - name: Install JMeter
        run: |
          wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${{ env.JMETER_VERSION }}.zip
          unzip apache-jmeter-${{ env.JMETER_VERSION }}.zip
          sudo mv apache-jmeter-${{ env.JMETER_VERSION }} /opt/jmeter
          sudo ln -s /opt/jmeter/bin/jmeter /usr/local/bin/jmeter
          jmeter --version

      - name: Create test data directory
        run: |
          mkdir -p backend/jmeter/results

      - name: Verify test data exists
        working-directory: ./backend
        run: |
          if [ ! -f "jmeter/testdata/test_urls.csv" ]; then
            echo "❌ Error: test_urls.csv not found in jmeter/testdata/"
            echo "Please ensure the file exists in your repository"
            ls -la jmeter/testdata/ || echo "Directory doesn't exist"
            exit 1
          fi
          
          echo "✅ Test data file found"
          echo "Number of test URLs: $(tail -n +2 jmeter/testdata/test_urls.csv | wc -l)"
          echo "First 5 lines of test data:"
          head -5 jmeter/testdata/test_urls.csv

      - name: Create JMeter test plans directory
        working-directory: ./backend
        run: |
          mkdir -p jmeter/testplans

      - name: Run Load Tests
        working-directory: ./backend
        run: |
          # Configurar variables de entorno para JMeter
          export TEST_DURATION="${{ github.event.inputs.test_duration || '300' }}"
          export CONCURRENT_USERS="${{ github.event.inputs.concurrent_users || '50' }}"
          export RAMP_UP_TIME="${{ github.event.inputs.ramp_up_time || '60' }}"
          
          # Ejecutar pruebas de carga
          jmeter -n -t jmeter/testplans/url_shortener_load_test.jmx \
            -l jmeter/results/load_test_results.jtl \
            -e -o jmeter/results/html_report \
            -Jthreads=$CONCURRENT_USERS \
            -Jrampup=$RAMP_UP_TIME \
            -Jduration=$TEST_DURATION \
            -Jhost=localhost \
            -Jport=8081 \
            -Jprotocol=http

      - name: Run Stress Tests
        working-directory: ./backend
        run: |
          # Prueba de estrés con más usuarios
          STRESS_USERS=100
          STRESS_RAMP_UP=30
          STRESS_DURATION=180
          
          jmeter -n -t jmeter/testplans/url_shortener_stress_test.jmx \
            -l jmeter/results/stress_test_results.jtl \
            -e -o jmeter/results/stress_html_report \
            -Jthreads=$STRESS_USERS \
            -Jrampup=$STRESS_RAMP_UP \
            -Jduration=$STRESS_DURATION \
            -Jhost=localhost \
            -Jport=8081 \
            -Jprotocol=http

      - name: Run Spike Tests
        working-directory: ./backend
        run: |
          # Prueba de picos de carga
          SPIKE_USERS=200
          SPIKE_RAMP_UP=10
          SPIKE_DURATION=60
          
          jmeter -n -t jmeter/testplans/url_shortener_spike_test.jmx \
            -l jmeter/results/spike_test_results.jtl \
            -e -o jmeter/results/spike_html_report \
            -Jthreads=$SPIKE_USERS \
            -Jrampup=$SPIKE_RAMP_UP \
            -Jduration=$SPIKE_DURATION \
            -Jhost=localhost \
            -Jport=8081 \
            -Jprotocol=http

      - name: Analyze Results
        working-directory: ./backend
        run: |
          echo "=== LOAD TEST RESULTS ===" 
          if [ -f "jmeter/results/load_test_results.jtl" ]; then
            # Mostrar estadísticas básicas
            awk -F',' 'NR>1 {
              total++; 
              if($8=="true") success++; 
              if($8=="false") errors++;
              sum+=$2; 
              if($2>max || max=="") max=$2; 
              if($2<min || min=="") min=$2
            } 
            END {
              print "Total Requests: " total;
              print "Successful: " success " (" (success/total*100) "%)";
              print "Errors: " errors " (" (errors/total*100) "%)";
              print "Average Response Time: " (sum/total) " ms";
              print "Min Response Time: " min " ms";
              print "Max Response Time: " max " ms"
            }' jmeter/results/load_test_results.jtl
          fi
          
          echo -e "\n=== STRESS TEST RESULTS ===" 
          if [ -f "jmeter/results/stress_test_results.jtl" ]; then
            awk -F',' 'NR>1 {
              total++; 
              if($8=="true") success++; 
              if($8=="false") errors++;
              sum+=$2; 
              if($2>max || max=="") max=$2; 
              if($2<min || min=="") min=$2
            } 
            END {
              print "Total Requests: " total;
              print "Successful: " success " (" (success/total*100) "%)";
              print "Errors: " errors " (" (errors/total*100) "%)";
              print "Average Response Time: " (sum/total) " ms";
              print "Min Response Time: " min " ms";
              print "Max Response Time: " max " ms"
            }' jmeter/results/stress_test_results.jtl
          fi
          
          echo -e "\n=== SPIKE TEST RESULTS ===" 
          if [ -f "jmeter/results/spike_test_results.jtl" ]; then
            awk -F',' 'NR>1 {
              total++; 
              if($8=="true") success++; 
              if($8=="false") errors++;
              sum+=$2; 
              if($2>max || max=="") max=$2; 
              if($2<min || min=="") min=$2
            } 
            END {
              print "Total Requests: " total;
              print "Successful: " success " (" (success/total*100) "%)";
              print "Errors: " errors " (" (errors/total*100) "%)";
              print "Average Response Time: " (sum/total) " ms";
              print "Min Response Time: " min " ms";
              print "Max Response Time: " max " ms"
            }' jmeter/results/spike_test_results.jtl
          fi

      - name: Check Performance Thresholds
        working-directory: ./backend
        run: |
          # Verificar que el porcentaje de errores sea menor al 5%
          ERROR_THRESHOLD=5
          RESPONSE_TIME_THRESHOLD=2000
          
          for test_type in "load" "stress" "spike"; do
            if [ -f "jmeter/results/${test_type}_test_results.jtl" ]; then
              ERROR_RATE=$(awk -F',' 'NR>1 {total++; if($8=="false") errors++} END {print (errors/total*100)}' jmeter/results/${test_type}_test_results.jtl)
              AVG_RESPONSE_TIME=$(awk -F',' 'NR>1 {sum+=$2; count++} END {print sum/count}' jmeter/results/${test_type}_test_results.jtl)
          
              echo "Test: ${test_type}"
              echo "Error Rate: ${ERROR_RATE}%"
              echo "Average Response Time: ${AVG_RESPONSE_TIME}ms"
          
              if (( $(echo "$ERROR_RATE > $ERROR_THRESHOLD" | bc -l) )); then
                echo "❌ ERROR: Error rate (${ERROR_RATE}%) exceeds threshold (${ERROR_THRESHOLD}%)"
                exit 1
              fi
          
              if (( $(echo "$AVG_RESPONSE_TIME > $RESPONSE_TIME_THRESHOLD" | bc -l) )); then
                echo "⚠️  WARNING: Average response time (${AVG_RESPONSE_TIME}ms) exceeds threshold (${RESPONSE_TIME_THRESHOLD}ms)"
              fi
          
              echo "✅ ${test_type} test passed performance thresholds"
              echo "---"
            fi
          done

      - name: Upload JMeter Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jmeter-results
          path: |
            backend/jmeter/results/
          retention-days: 30

      - name: Upload HTML Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jmeter-html-reports
          path: |
            backend/jmeter/results/html_report/
            backend/jmeter/results/stress_html_report/
            backend/jmeter/results/spike_html_report/
          retention-days: 30

      - name: Upload Docker logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: docker-logs
          path: |
            docker-logs.txt
          retention-days: 7

      - name: Stop Docker services
        if: always()
        run: |
          echo "Stopping Docker services..."
          docker compose -f docker-compose.test.yml logs backend > docker-logs.txt || true
          docker compose -f docker-compose.test.yml down -v || true